# PrivML-FHE

ENGR489 Project: Implementation of Privacy Preserving Machine Learning Model Using Homomorphic Encryption

INFO: This is an in progress repository. It is not regularly updated due to Git implementations running from ECS Gitlab. Will be periodically updated. FHE workflow is not yet integrated and is in progress.

## Introduction

With growing concerns around data privacy and collecting sensitive information, the ability to process data securely without exposing its underlying form has become a key engineering challenge. As machine learning (ML) models become increasingly integrated into business sectors, their reliance on raw data raises significant privacy and security risks.

This project proposes developing a privacy-preserving ML system using Fully Homomorphic Encryption (FHE), a cryptographic method that enables computations to be performed directly on encrypted data [1]. The aim is to investigate whether an end-to-end solution, where a selected ML model is trained and used for inference without decrypting input data, is feasible and practical under current constraints.

The project will be evaluated by comparing the performance and accuracy of the encrypted model with an equivalent unencrypted version. Exploration into real-world use of FHE will be performed once requirements have been met, with a demonstration with a simple UI to show how this technology could be applied in practice.

## The Problem

As machine learning (ML) becomes increasingly integrated into business operations and decision-making, concerns around privacy and protection are crucial to address [2]. ML models require vast amounts of data to function effectively, much of which may include sensitive or proprietary information such as personally identifiable information (PII), protected health information (PHI), intellectual property (IP), and confidential business data.
Organisations may be hesitant to expose this data in plaintext form during training and inference, especially in sectors like healthcare, finance, and defence, where data breaches can lead to severe legal, financial, and reputational consequences.

From a privacy and rights perspective, the current landscape of AI development often lacks transparency and proper consent [3]. Large-scale AI systems are frequently trained on datasets scraped from public and privacy sources, sometimes without authorisation or regard for data ownership [4]. This raises serious ethical and legal concerns, particularly when copyrighted or sensitive content is used without proper permission or handled without appropriate security practices. In addition to legal and reputational risks, the lack of data privacy opens up new vectors for cyberattacks. Adversaries may target ML systems by injecting malicious training data, stealing sensitive information, or exploiting vulnerabilities during data transmission and storage. These risks highlight the urgent need for secure computation methods that eliminate the exposure of raw inputs throughout the ML lifecycle.

To maintain trust, comply with legal/regulatory requirements, and reduce the risk of exposure, it is essential to explore new approaches that enable secure computation of sensitive data. A system that enables ML workflows to preserve the confidentiality of input data during training and inference would represent a significant step forward in responsible AI development. Investigating the feasibility, performance and practicality of such methods offers a pathway toward more secure, ethical, and deployable ML systems in real-world environments.

## Solution

The proposed solution involves developing a privacy-preserving machine learning (ML) pipeline that uses Fully Homomorphic Encryption (FHE) to operate directly on encrypted data. FHE is an advanced cryptographic technique that enables computations to be performed on ciphertexts, producing encrypted results that, when decrypted, match the output of the same operations on the original plaintext [1]. In other words, it allows data to remain encrypted throughout the entire computational process, eliminating the need to expose raw inputs at any stage. This property makes FHE especially attractive for applications where data confidentiality and user privacy are important. FHE supports confidentiality, integrity, and privacy principles, offering a strong foundation for secure ML workflows. A selection of basic ML models, such as logistic regression, linear regression, and k-nearest neighbour, will be evaluated to determine
which algorithms are most compatible with the computational constraints of FHE.

Despite its promise, FHE has yet to see widespread adoption in real-world ML systems due to its significant performance overhead, complex implementation, and limited library support. Trivial operations in plaintext become significantly more computationally expensive and restrictive when performed over encrypted data. To explore the trade-off between privacy and practicality, this project will also examine adjustable security parameters, such as key lengths
or encryption schemes, to evaluate whether reducing secure practices would improve performance during prototyping. These controlled compromises will help assess the feasibility of integrating FHE into ML workflows under constrained conditions, offering insight into its potential for future real-world applications.

To implement the solution, an analysis of open-source libraries and repositories will be studied, and selection will be based on ease of integration and support of ML operations. A baseline ML model will first be implemented using plaintext data to establish benchmarks for performance and accuracy. The same model will then be recreated using encrypted data and FHE to perform training and inference. Publicly available or synthetic datasets will be used to test and evaluate the system. A simple user interface may also be developed to demonstrate how FHE-backed systems could be integrated into real-world software and made accessible to broader, possibly non-technical audiences.

## Evaluation

The evaluation of this project will focus on both the technical performance and practical viability of the implemented system. Since the goal is to explore the feasibility of a privacy-preserving ML pipeline using encrypted data, the evaluation will assess how effectively the system preserves privacy while delivering meaningful and accurate model outputs. The evaluation will be structured around three key areas:

1. Correctness and Functionality: The primary objective is to ensure the encrypted ML model behaves as expected. Predictions on encrypted data should closely align with those of an equivalent plaintext model. This will involve unit testing of encryption routines, validating model outputs, and conducting consistency checks between encrypted and unencrypted workflows to confirm functional correctness.
1. Performance and Trade-Off Analysis: The encrypted model will be benchmarked against a baseline unencrypted version to evaluate runtime performance, memory usage, model accuracy, and encryption/decryption overhead. Based on these results, parameters such as key size, ciphertext modulus, or model architecture (e.g. linear regression vs. k-nearest-neighbour) may be adjusted to optimise performance while maintaining reasonable privacy and security standards. These trade-offs are an essential part of assessing FHE’s practicality in real-world scenarios.
1. Usability and Real-World Demonstration: The system will also be evaluated in terms of usability and applicability. A basic user interface may be developed to simulate how the encrypted model could function in a real-world software environment. This will support evaluation of the system’s accessibility, particularly for non-technical users, and serve as a communication tool to demonstrate the potential integration of privacy-preserving ML in business applications.

The evaluation process will be iterative, with findings from each phase informing potential changes in direction. For instance, if performance is insufficient, the project may pivot to a simpler ML model, or adjust encryption parameters to improve speed while balancing privacy. This flexible and exploratory approach ensures the final outcome is not only technically sound but also provides a realistic assessment of the viability of FHE-based ML in practical settings.

## Software, Datasets, and ML models

This project will involve a range of specialised software libraries and frameworks to support development. As the project is still in the early stages, specific tools, libraries, and frameworks will be selected based on factors such as ease of integration, performance, documentation, and community support. For FHE implementation, the project will explore modern FHE open-source Python and C++ libraries and repositories such as PySeal, TenSEAL, Helib. Other open-source FHE libraries or GitHub repositories may also be considered depending on implementation needs. For ML implementation, well-used and supported libraries such as scikit-learn, TensorFlow, PyTorch, or Keras will be explored, as each offers its own benefits regarding the simplicity and complexity of component and model tuning capabilities. Alternatively, some models can be implemented from scratch in Python to allow full control over encrypted operations and integration. For dataset selection, publicly available datasets may be used for testing and evaluation, depending on model requirements. Options such as
Iris, Titanic, or MNIST datasets can be investigated, and other platforms such as Kaggle, OpenML, or UCI ML Repository offer a more simplistic, interpretable, and computationally cheap approach. Dataset selection will remain flexible, due to that not being the projects focus.
